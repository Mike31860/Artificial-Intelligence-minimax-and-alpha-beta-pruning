{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SfayRXTLqi0f"
   },
   "source": [
    "# Neural Networks with scikit-learn\n",
    "\n",
    "Neural networks are amongst the most complex and flexible machine/deep learning models, due to this, their capability to tackle more complex problems is huge. However, due to this flexibility, they are also easier to overfit, which is why it's the data scientists job to find the correct hyper-parameters for these models.\n",
    "\n",
    "In order to do this, we'll use scikit learn in the same way we always have. In this case, however, neural networks have a lot more than 1 parameter, in case of the multi-layer perceptron models in scikit-learn the parameters are:\n",
    "\n",
    "* hidden_layer_sizes\n",
    "* activation\n",
    "* solver\n",
    "* alpha\n",
    "* batch_size\n",
    "* learning_rate\n",
    "* learning_rate_init\n",
    "* power_t\n",
    "* max_iter\n",
    "* shuffle\n",
    "* random_state\n",
    "* tol\n",
    "* verbose\n",
    "* warm_start\n",
    "* momentum\n",
    "* nesterovs_momentum\n",
    "* early_stopping\n",
    "* validation_fraction\n",
    "* beta_1\n",
    "* beta_2\n",
    "* epsilon\n",
    "* n_iter_no_change\n",
    "\n",
    "You can read more about each one [here](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html).\n",
    "\n",
    "## The data (MNIST)\n",
    "\n",
    "Let's load the digit data from MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bl4QgjATp2Nk"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "data = pd.read_csv(\"train.csv\")\n",
    "test_data = pd.read_csv(\"test.csv\") # Only for Kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We visualize the already familiar data with the modified label type to category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel0</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel774</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n",
       "0     1       0       0       0       0       0       0       0       0   \n",
       "1     0       0       0       0       0       0       0       0       0   \n",
       "2     1       0       0       0       0       0       0       0       0   \n",
       "3     4       0       0       0       0       0       0       0       0   \n",
       "4     0       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   pixel8  ...  pixel774  pixel775  pixel776  pixel777  pixel778  pixel779  \\\n",
       "0       0  ...         0         0         0         0         0         0   \n",
       "1       0  ...         0         0         0         0         0         0   \n",
       "2       0  ...         0         0         0         0         0         0   \n",
       "3       0  ...         0         0         0         0         0         0   \n",
       "4       0  ...         0         0         0         0         0         0   \n",
       "\n",
       "   pixel780  pixel781  pixel782  pixel783  \n",
       "0         0         0         0         0  \n",
       "1         0         0         0         0  \n",
       "2         0         0         0         0  \n",
       "3         0         0         0         0  \n",
       "4         0         0         0         0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.label = data.label.astype(\"category\")\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the data set into a test and training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(data.drop(\"label\", axis = 1), data.label, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The network\n",
    "\n",
    "This classifier is a dense neural network, which can be visualized as:\n",
    "\n",
    "![By Glosser.ca - Own work, Derivative of File:Artificial neural network.svg, CC BY-SA 3.0, https://commons.wikimedia.org/w/index.php?curid=24913461](resources/nnet.png)\n",
    "\n",
    "This network architecture is the **MLPClassifier** in scikit-learn, first we will try a network with one hidden with default parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 10.26146455\n",
      "Iteration 2, loss = 1.83843986\n",
      "Iteration 3, loss = 1.04772283\n",
      "Iteration 4, loss = 0.67763113\n",
      "Iteration 5, loss = 0.46146341\n",
      "Iteration 6, loss = 0.34692434\n",
      "Iteration 7, loss = 0.26143220\n",
      "Iteration 8, loss = 0.20229604\n",
      "Iteration 9, loss = 0.16285930\n",
      "Iteration 10, loss = 0.13975919\n",
      "Iteration 11, loss = 0.11227159\n",
      "Iteration 12, loss = 0.11205778\n",
      "Iteration 13, loss = 0.08230610\n",
      "Iteration 14, loss = 0.07142090\n",
      "Iteration 15, loss = 0.07541603\n",
      "Iteration 16, loss = 0.07819111\n",
      "Iteration 17, loss = 0.08094314\n",
      "Iteration 18, loss = 0.07275204\n",
      "Iteration 19, loss = 0.07556990\n",
      "Iteration 20, loss = 0.06547626\n",
      "Iteration 21, loss = 0.07011028\n",
      "Iteration 22, loss = 0.07428769\n",
      "Iteration 23, loss = 0.05611521\n",
      "Iteration 24, loss = 0.06898180\n",
      "Iteration 25, loss = 0.09274774\n",
      "Iteration 26, loss = 0.09507757\n",
      "Iteration 27, loss = 0.09063448\n",
      "Iteration 28, loss = 0.07772569\n",
      "Iteration 29, loss = 0.07285178\n",
      "Iteration 30, loss = 0.06775419\n",
      "Iteration 31, loss = 0.07997448\n",
      "Iteration 32, loss = 0.06292833\n",
      "Iteration 33, loss = 0.06026829\n",
      "Iteration 34, loss = 0.06463138\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "              hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "              learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "              n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "              random_state=None, shuffle=True, solver='adam', tol=0.0001,\n",
       "              validation_fraction=0.1, verbose=True, warm_start=False)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "net = MLPClassifier(verbose = True)\n",
    "net.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the default parameters above after the description of the epochs (iterations).\n",
    "\n",
    "Now we can visualize it's accuracy in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.950952380952381"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of the box, this model will give us an accuracy of 95-96%. Let's explore some basic parameters and what they mean.\n",
    "\n",
    "### Learning rate (learning_rate_init)\n",
    "\n",
    "This parameter controls the size of the \"steps\" a network takes when performing gradient descent, if this is too low, the network will take very long to converge, if it's too high, it might diverge instead, let's see some examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------Starting training with lr 1e-06\n",
      "Iteration 1, loss = inf\n",
      "Iteration 2, loss = 156.24378280\n",
      "Iteration 3, loss = 145.88952965\n",
      "Iteration 4, loss = 136.65493102\n",
      "Iteration 5, loss = 128.35060788\n",
      "Iteration 6, loss = 120.84511087\n",
      "Iteration 7, loss = 114.02259303\n",
      "Iteration 8, loss = 107.78673363\n",
      "Iteration 9, loss = 102.07041656\n",
      "Iteration 10, loss = 96.79393813\n",
      "Iteration 11, loss = 91.88569928\n",
      "Iteration 12, loss = 87.28857803\n",
      "Iteration 13, loss = 82.99309977\n",
      "Iteration 14, loss = 78.96251148\n",
      "Iteration 15, loss = 75.15411581\n",
      "Iteration 16, loss = 71.55755806\n",
      "Iteration 17, loss = 68.17757104\n",
      "Iteration 18, loss = 65.00437283\n",
      "Iteration 19, loss = 62.02097122\n",
      "Iteration 20, loss = 59.22087523\n",
      "Iteration 21, loss = 56.59936060\n",
      "Iteration 22, loss = 54.14548791\n",
      "Iteration 23, loss = 51.83451477\n",
      "Iteration 24, loss = 49.65551517\n",
      "Iteration 25, loss = 47.59807189\n",
      "Iteration 26, loss = 45.64806810\n",
      "Iteration 27, loss = 43.80259151\n",
      "Iteration 28, loss = 42.06786180\n",
      "Iteration 29, loss = 40.43697886\n",
      "Iteration 30, loss = 38.90186690\n",
      "Iteration 31, loss = 37.45888677\n",
      "Iteration 32, loss = 36.09328412\n",
      "Iteration 33, loss = 34.80210112\n",
      "Iteration 34, loss = 33.57649017\n",
      "Iteration 35, loss = 32.41638328\n",
      "Iteration 36, loss = 31.31890322\n",
      "Iteration 37, loss = 30.28405002\n",
      "Iteration 38, loss = 29.30864823\n",
      "Iteration 39, loss = 28.38824344\n",
      "Iteration 40, loss = 27.51816041\n",
      "Iteration 41, loss = 26.69655471\n",
      "Iteration 42, loss = 25.91914097\n",
      "Iteration 43, loss = 25.17950043\n",
      "Iteration 44, loss = 24.47780930\n",
      "Iteration 45, loss = 23.81128295\n",
      "Iteration 46, loss = 23.17938186\n",
      "Iteration 47, loss = 22.57819796\n",
      "Iteration 48, loss = 22.00609693\n",
      "Iteration 49, loss = 21.46108304\n",
      "Iteration 50, loss = 20.94262653\n",
      "Iteration 51, loss = 20.44896970\n",
      "Iteration 52, loss = 19.97904390\n",
      "Iteration 53, loss = 19.53077219\n",
      "Iteration 54, loss = 19.10164136\n",
      "Iteration 55, loss = 18.69073552\n",
      "Iteration 56, loss = 18.29677983\n",
      "Iteration 57, loss = 17.91886024\n",
      "Iteration 58, loss = 17.55575555\n",
      "Iteration 59, loss = 17.20859056\n",
      "Iteration 60, loss = 16.87447071\n",
      "Iteration 61, loss = 16.55364081\n",
      "Iteration 62, loss = 16.24495502\n",
      "Iteration 63, loss = 15.94855149\n",
      "Iteration 64, loss = 15.66366262\n",
      "Iteration 65, loss = 15.38898932\n",
      "Iteration 66, loss = 15.12470321\n",
      "Iteration 67, loss = 14.87063489\n",
      "Iteration 68, loss = 14.62488200\n",
      "Iteration 69, loss = 14.38827240\n",
      "Iteration 70, loss = 14.16054424\n",
      "Iteration 71, loss = 13.93984165\n",
      "Iteration 72, loss = 13.72638408\n",
      "Iteration 73, loss = 13.52010233\n",
      "Iteration 74, loss = 13.31940715\n",
      "Iteration 75, loss = 13.12574860\n",
      "Iteration 76, loss = 12.93796175\n",
      "Iteration 77, loss = 12.75675639\n",
      "Iteration 78, loss = 12.58075730\n",
      "Iteration 79, loss = 12.41160109\n",
      "Iteration 80, loss = 12.24716024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Felipe\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (80) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------Finished lr 1e-06 with score 0.7071428571428572\n",
      "-----------Starting training with lr 1e-05\n",
      "Iteration 1, loss = 120.38953030\n",
      "Iteration 2, loss = 72.39161349\n",
      "Iteration 3, loss = 50.07755297\n",
      "Iteration 4, loss = 37.86612161\n",
      "Iteration 5, loss = 30.16467350\n",
      "Iteration 6, loss = 24.97445868\n",
      "Iteration 7, loss = 21.32192646\n",
      "Iteration 8, loss = 18.62356289\n",
      "Iteration 9, loss = 16.53949361\n",
      "Iteration 10, loss = 14.89907361\n",
      "Iteration 11, loss = 13.56506011\n",
      "Iteration 12, loss = 12.45956299\n",
      "Iteration 13, loss = 11.52444344\n",
      "Iteration 14, loss = 10.72148254\n",
      "Iteration 15, loss = 10.02897873\n",
      "Iteration 16, loss = 9.41851140\n",
      "Iteration 17, loss = 8.87404891\n",
      "Iteration 18, loss = 8.39365407\n",
      "Iteration 19, loss = 7.95478608\n",
      "Iteration 20, loss = 7.55543364\n",
      "Iteration 21, loss = 7.19088843\n",
      "Iteration 22, loss = 6.86566811\n",
      "Iteration 23, loss = 6.56814635\n",
      "Iteration 24, loss = 6.29650767\n",
      "Iteration 25, loss = 6.04073482\n",
      "Iteration 26, loss = 5.80400939\n",
      "Iteration 27, loss = 5.58546586\n",
      "Iteration 28, loss = 5.37907454\n",
      "Iteration 29, loss = 5.18545751\n",
      "Iteration 30, loss = 4.99791454\n",
      "Iteration 31, loss = 4.83033016\n",
      "Iteration 32, loss = 4.66638352\n",
      "Iteration 33, loss = 4.51801856\n",
      "Iteration 34, loss = 4.37691468\n",
      "Iteration 35, loss = 4.24352298\n",
      "Iteration 36, loss = 4.11805502\n",
      "Iteration 37, loss = 3.99742213\n",
      "Iteration 38, loss = 3.88332886\n",
      "Iteration 39, loss = 3.77725183\n",
      "Iteration 40, loss = 3.66889674\n",
      "Iteration 41, loss = 3.57065180\n",
      "Iteration 42, loss = 3.47432790\n",
      "Iteration 43, loss = 3.38103345\n",
      "Iteration 44, loss = 3.29426237\n",
      "Iteration 45, loss = 3.21080052\n",
      "Iteration 46, loss = 3.12998584\n",
      "Iteration 47, loss = 3.05051525\n",
      "Iteration 48, loss = 2.97859225\n",
      "Iteration 49, loss = 2.90395004\n",
      "Iteration 50, loss = 2.83228392\n",
      "Iteration 51, loss = 2.76569280\n",
      "Iteration 52, loss = 2.69822860\n",
      "Iteration 53, loss = 2.63570939\n",
      "Iteration 54, loss = 2.57643997\n",
      "Iteration 55, loss = 2.51888318\n",
      "Iteration 56, loss = 2.46007308\n",
      "Iteration 57, loss = 2.40523298\n",
      "Iteration 58, loss = 2.35265826\n",
      "Iteration 59, loss = 2.30021348\n",
      "Iteration 60, loss = 2.24701088\n",
      "Iteration 61, loss = 2.20003712\n",
      "Iteration 62, loss = 2.15060019\n",
      "Iteration 63, loss = 2.10477162\n",
      "Iteration 64, loss = 2.05831982\n",
      "Iteration 65, loss = 2.01485045\n",
      "Iteration 66, loss = 1.97278480\n",
      "Iteration 67, loss = 1.93132995\n",
      "Iteration 68, loss = 1.88883137\n",
      "Iteration 69, loss = 1.85018386\n",
      "Iteration 70, loss = 1.81136523\n",
      "Iteration 71, loss = 1.77443130\n",
      "Iteration 72, loss = 1.73898488\n",
      "Iteration 73, loss = 1.70245764\n",
      "Iteration 74, loss = 1.67086569\n",
      "Iteration 75, loss = 1.63461824\n",
      "Iteration 76, loss = 1.60124429\n",
      "Iteration 77, loss = 1.57011561\n",
      "Iteration 78, loss = 1.54190874\n",
      "Iteration 79, loss = 1.50864878\n",
      "Iteration 80, loss = 1.47926643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Felipe\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (80) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------Finished lr 1e-05 with score 0.8918253968253969\n",
      "-----------Starting training with lr 0.0001\n",
      "Iteration 1, loss = 39.82718742\n",
      "Iteration 2, loss = 9.60506738\n",
      "Iteration 3, loss = 6.34727149\n",
      "Iteration 4, loss = 4.81116778\n",
      "Iteration 5, loss = 3.89284095\n",
      "Iteration 6, loss = 3.23600823\n",
      "Iteration 7, loss = 2.75015119\n",
      "Iteration 8, loss = 2.37951866\n",
      "Iteration 9, loss = 2.08290071\n",
      "Iteration 10, loss = 1.83393093\n",
      "Iteration 11, loss = 1.60649796\n",
      "Iteration 12, loss = 1.42051504\n",
      "Iteration 13, loss = 1.25654700\n",
      "Iteration 14, loss = 1.12278612\n",
      "Iteration 15, loss = 0.98957145\n",
      "Iteration 16, loss = 0.88496243\n",
      "Iteration 17, loss = 0.77112971\n",
      "Iteration 18, loss = 0.68529236\n",
      "Iteration 19, loss = 0.60306707\n",
      "Iteration 20, loss = 0.53637999\n",
      "Iteration 21, loss = 0.47121709\n",
      "Iteration 22, loss = 0.41968900\n",
      "Iteration 23, loss = 0.37116154\n",
      "Iteration 24, loss = 0.32409773\n",
      "Iteration 25, loss = 0.28549747\n",
      "Iteration 26, loss = 0.25436936\n",
      "Iteration 27, loss = 0.22106471\n",
      "Iteration 28, loss = 0.19283029\n",
      "Iteration 29, loss = 0.16840343\n",
      "Iteration 30, loss = 0.14298109\n",
      "Iteration 31, loss = 0.12569919\n",
      "Iteration 32, loss = 0.11831637\n",
      "Iteration 33, loss = 0.09730830\n",
      "Iteration 34, loss = 0.08008382\n",
      "Iteration 35, loss = 0.07248529\n",
      "Iteration 36, loss = 0.06310622\n",
      "Iteration 37, loss = 0.05290504\n",
      "Iteration 38, loss = 0.04455528\n",
      "Iteration 39, loss = 0.04020763\n",
      "Iteration 40, loss = 0.03049615\n",
      "Iteration 41, loss = 0.03024103\n",
      "Iteration 42, loss = 0.02539303\n",
      "Iteration 43, loss = 0.01924954\n",
      "Iteration 44, loss = 0.01894026\n",
      "Iteration 45, loss = 0.01695139\n",
      "Iteration 46, loss = 0.01983312\n",
      "Iteration 47, loss = 0.01676850\n",
      "Iteration 48, loss = 0.01299113\n",
      "Iteration 49, loss = 0.01393872\n",
      "Iteration 50, loss = 0.01466869\n",
      "Iteration 51, loss = 0.01343406\n",
      "Iteration 52, loss = 0.01635083\n",
      "Iteration 53, loss = 0.00932785\n",
      "Iteration 54, loss = 0.00867784\n",
      "Iteration 55, loss = 0.00529297\n",
      "Iteration 56, loss = 0.00648043\n",
      "Iteration 57, loss = 0.00741736\n",
      "Iteration 58, loss = 0.01397274\n",
      "Iteration 59, loss = 0.01664471\n",
      "Iteration 60, loss = 0.02526841\n",
      "Iteration 61, loss = 0.01746471\n",
      "Iteration 62, loss = 0.00939373\n",
      "Iteration 63, loss = 0.00856973\n",
      "Iteration 64, loss = 0.01159014\n",
      "Iteration 65, loss = 0.00610586\n",
      "Iteration 66, loss = 0.00352697\n",
      "Iteration 67, loss = 0.00739210\n",
      "Iteration 68, loss = 0.00654815\n",
      "Iteration 69, loss = 0.01179607\n",
      "Iteration 70, loss = 0.01566193\n",
      "Iteration 71, loss = 0.01193465\n",
      "Iteration 72, loss = 0.00955475\n",
      "Iteration 73, loss = 0.00532562\n",
      "Iteration 74, loss = 0.00399159\n",
      "Iteration 75, loss = 0.00253604\n",
      "Iteration 76, loss = 0.00444272\n",
      "Iteration 77, loss = 0.01108489\n",
      "Iteration 78, loss = 0.01102134\n",
      "Iteration 79, loss = 0.01904538\n",
      "Iteration 80, loss = 0.01132333\n",
      "-----------Finished lr 0.0001 with score 0.9374603174603174\n",
      "-----------Starting training with lr 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Felipe\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (80) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 9.39810668\n",
      "Iteration 2, loss = 1.87911905\n",
      "Iteration 3, loss = 1.02345366\n",
      "Iteration 4, loss = 0.64248040\n",
      "Iteration 5, loss = 0.44456914\n",
      "Iteration 6, loss = 0.31535503\n",
      "Iteration 7, loss = 0.23779426\n",
      "Iteration 8, loss = 0.17982226\n",
      "Iteration 9, loss = 0.15677850\n",
      "Iteration 10, loss = 0.11214531\n",
      "Iteration 11, loss = 0.09188056\n",
      "Iteration 12, loss = 0.08868267\n",
      "Iteration 13, loss = 0.08221899\n",
      "Iteration 14, loss = 0.08372344\n",
      "Iteration 15, loss = 0.08399587\n",
      "Iteration 16, loss = 0.07581939\n",
      "Iteration 17, loss = 0.07074609\n",
      "Iteration 18, loss = 0.07462386\n",
      "Iteration 19, loss = 0.08444302\n",
      "Iteration 20, loss = 0.06912944\n",
      "Iteration 21, loss = 0.06895907\n",
      "Iteration 22, loss = 0.08391247\n",
      "Iteration 23, loss = 0.10152134\n",
      "Iteration 24, loss = 0.08389736\n",
      "Iteration 25, loss = 0.08279019\n",
      "Iteration 26, loss = 0.07106654\n",
      "Iteration 27, loss = 0.05504027\n",
      "Iteration 28, loss = 0.05303933\n",
      "Iteration 29, loss = 0.07265740\n",
      "Iteration 30, loss = 0.07223835\n",
      "Iteration 31, loss = 0.09674595\n",
      "Iteration 32, loss = 0.07053791\n",
      "Iteration 33, loss = 0.07067170\n",
      "Iteration 34, loss = 0.08822373\n",
      "Iteration 35, loss = 0.06594224\n",
      "Iteration 36, loss = 0.05604984\n",
      "Iteration 37, loss = 0.06084685\n",
      "Iteration 38, loss = 0.04938269\n",
      "Iteration 39, loss = 0.06489799\n",
      "Iteration 40, loss = 0.07360658\n",
      "Iteration 41, loss = 0.07909654\n",
      "Iteration 42, loss = 0.06319356\n",
      "Iteration 43, loss = 0.08181777\n",
      "Iteration 44, loss = 0.04874332\n",
      "Iteration 45, loss = 0.04746743\n",
      "Iteration 46, loss = 0.05385245\n",
      "Iteration 47, loss = 0.06200845\n",
      "Iteration 48, loss = 0.04278669\n",
      "Iteration 49, loss = 0.03598498\n",
      "Iteration 50, loss = 0.04583847\n",
      "Iteration 51, loss = 0.07511964\n",
      "Iteration 52, loss = 0.08372303\n",
      "Iteration 53, loss = 0.06289610\n",
      "Iteration 54, loss = 0.06729825\n",
      "Iteration 55, loss = 0.03634438\n",
      "Iteration 56, loss = 0.03033596\n",
      "Iteration 57, loss = 0.03743100\n",
      "Iteration 58, loss = 0.04655808\n",
      "Iteration 59, loss = 0.03081520\n",
      "Iteration 60, loss = 0.02998386\n",
      "Iteration 61, loss = 0.03310774\n",
      "Iteration 62, loss = 0.03932426\n",
      "Iteration 63, loss = 0.04933758\n",
      "Iteration 64, loss = 0.08128092\n",
      "Iteration 65, loss = 0.05741652\n",
      "Iteration 66, loss = 0.05476541\n",
      "Iteration 67, loss = 0.04829471\n",
      "Iteration 68, loss = 0.04612688\n",
      "Iteration 69, loss = 0.06282760\n",
      "Iteration 70, loss = 0.04389823\n",
      "Iteration 71, loss = 0.03507656\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "-----------Finished lr 0.001 with score 0.9616666666666667\n",
      "-----------Starting training with lr 0.01\n",
      "Iteration 1, loss = inf\n",
      "Iteration 2, loss = 0.86651761\n",
      "Iteration 3, loss = 0.64584270\n",
      "Iteration 4, loss = 0.54474773\n",
      "Iteration 5, loss = 0.47105300\n",
      "Iteration 6, loss = 0.41965236\n",
      "Iteration 7, loss = 0.40025812\n",
      "Iteration 8, loss = 0.40270662\n",
      "Iteration 9, loss = 0.40830732\n",
      "Iteration 10, loss = 0.36507515\n",
      "Iteration 11, loss = 0.36344270\n",
      "Iteration 12, loss = 0.39136028\n",
      "Iteration 13, loss = 0.41989574\n",
      "Iteration 14, loss = 0.42805878\n",
      "Iteration 15, loss = 0.42153448\n",
      "Iteration 16, loss = 0.46177419\n",
      "Iteration 17, loss = 0.42853564\n",
      "Iteration 18, loss = 0.42532276\n",
      "Iteration 19, loss = 0.42426628\n",
      "Iteration 20, loss = 0.42979149\n",
      "Iteration 21, loss = 0.46119923\n",
      "Iteration 22, loss = 0.52389999\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "-----------Finished lr 0.01 with score 0.8629365079365079\n",
      "-----------Starting training with lr 0.1\n",
      "Iteration 1, loss = inf\n",
      "Iteration 2, loss = 2.23819592\n",
      "Iteration 3, loss = inf\n",
      "Iteration 4, loss = 2.23954122\n",
      "Iteration 5, loss = 2.27921601\n",
      "Iteration 6, loss = 2.29081377\n",
      "Iteration 7, loss = 2.27457980\n",
      "Iteration 8, loss = 2.27178261\n",
      "Iteration 9, loss = 2.28116236\n",
      "Iteration 10, loss = 2.27290080\n",
      "Iteration 11, loss = 2.26758691\n",
      "Iteration 12, loss = 2.26099664\n",
      "Iteration 13, loss = 2.25304669\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "-----------Finished lr 0.1 with score 0.14134920634920634\n",
      "-----------Starting training with lr 1\n",
      "Iteration 1, loss = inf\n",
      "Iteration 2, loss = inf\n",
      "Iteration 3, loss = 2.78880116\n",
      "Iteration 4, loss = 2.77895745\n",
      "Iteration 5, loss = 2.77702231\n",
      "Iteration 6, loss = 2.77435055\n",
      "Iteration 7, loss = 2.76758443\n",
      "Iteration 8, loss = 2.76515767\n",
      "Iteration 9, loss = 2.75652846\n",
      "Iteration 10, loss = 2.76331770\n",
      "Iteration 11, loss = 2.76027760\n",
      "Iteration 12, loss = 2.73729273\n",
      "Iteration 13, loss = 2.74311367\n",
      "Iteration 14, loss = 2.73758655\n",
      "Iteration 15, loss = 2.73458951\n",
      "Iteration 16, loss = 2.72562828\n",
      "Iteration 17, loss = 2.72089004\n",
      "Iteration 18, loss = 2.72700119\n",
      "Iteration 19, loss = 2.71724174\n",
      "Iteration 20, loss = 2.71131396\n",
      "Iteration 21, loss = 2.70235429\n",
      "Iteration 22, loss = inf\n",
      "Iteration 23, loss = 2.75026096\n",
      "Iteration 24, loss = 2.75057585\n",
      "Iteration 25, loss = 2.74392528\n",
      "Iteration 26, loss = 2.72760534\n",
      "Iteration 27, loss = 2.72661621\n",
      "Iteration 28, loss = 2.72027853\n",
      "Iteration 29, loss = 2.71548170\n",
      "Iteration 30, loss = 2.71151220\n",
      "Iteration 31, loss = 2.70904882\n",
      "Iteration 32, loss = 2.70757508\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "-----------Finished lr 1 with score 0.09746031746031746\n"
     ]
    }
   ],
   "source": [
    "learning_rates = [0.000001, 0.00001, 0.0001, 0.001, 0.01, 0.1, 1]\n",
    "scores = []\n",
    "\n",
    "for lr in learning_rates:\n",
    "    print(f\"-----------Starting training with lr {lr}\")\n",
    "    model = MLPClassifier(learning_rate_init = lr, verbose = True, max_iter=80)\n",
    "    model.fit(x_train, y_train)\n",
    "    score = model.score(x_test, y_test)\n",
    "    scores.append(score)\n",
    "    print(f\"-----------Finished lr {lr} with score {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x22781b2f438>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(learning_rates, scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that as the learning rate increases too much, accuracy immediately drops due to the model divergin, whereas when the learning rate is low, it has lower accuracy than the higher (but not too high) learning rates after it.\n",
    "\n",
    "Now let's try modifying the layers, we will try 1 hidden layer with 100 neurons (default), 2 hidden layers with 20 and 10 neurons, 3 hidden layers with 30, 20 and 10 neurons and 4 hidden layers with 100, 30, 20, 10 neurons. We will use a learning rate of 0.001 (default)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------Starting training with lr 100\n",
      "Iteration 1, loss = 9.77190092\n",
      "Iteration 2, loss = 1.84778452\n",
      "Iteration 3, loss = 1.06915906\n",
      "Iteration 4, loss = 0.65737945\n",
      "Iteration 5, loss = 0.47222375\n",
      "Iteration 6, loss = 0.33524773\n",
      "Iteration 7, loss = 0.23453425\n",
      "Iteration 8, loss = 0.19431840\n",
      "Iteration 9, loss = 0.15883297\n",
      "Iteration 10, loss = 0.11552765\n",
      "Iteration 11, loss = 0.10408286\n",
      "Iteration 12, loss = 0.08472054\n",
      "Iteration 13, loss = 0.08800138\n",
      "Iteration 14, loss = 0.07162908\n",
      "Iteration 15, loss = 0.07725046\n",
      "Iteration 16, loss = 0.11412627\n",
      "Iteration 17, loss = 0.13420784\n",
      "Iteration 18, loss = 0.11038861\n",
      "Iteration 19, loss = 0.09104652\n",
      "Iteration 20, loss = 0.08624993\n",
      "Iteration 21, loss = 0.09540535\n",
      "Iteration 22, loss = 0.07629342\n",
      "Iteration 23, loss = 0.09543054\n",
      "Iteration 24, loss = 0.09291153\n",
      "Iteration 25, loss = 0.07364293\n",
      "Iteration 26, loss = 0.07861657\n",
      "Iteration 27, loss = 0.08636618\n",
      "Iteration 28, loss = 0.08915543\n",
      "Iteration 29, loss = 0.07590769\n",
      "Iteration 30, loss = 0.07158298\n",
      "Iteration 31, loss = 0.06852743\n",
      "Iteration 32, loss = 0.06783627\n",
      "Iteration 33, loss = 0.07492589\n",
      "Iteration 34, loss = 0.08081333\n",
      "Iteration 35, loss = 0.06307085\n",
      "Iteration 36, loss = 0.07189716\n",
      "Iteration 37, loss = 0.06667163\n",
      "Iteration 38, loss = 0.07937246\n",
      "Iteration 39, loss = 0.07042300\n",
      "Iteration 40, loss = 0.06671112\n",
      "Iteration 41, loss = 0.08323598\n",
      "Iteration 42, loss = 0.06860775\n",
      "Iteration 43, loss = 0.05315238\n",
      "Iteration 44, loss = 0.06409655\n",
      "Iteration 45, loss = 0.08071254\n",
      "Iteration 46, loss = 0.06892368\n",
      "Iteration 47, loss = 0.07799089\n",
      "Iteration 48, loss = 0.05317450\n",
      "Iteration 49, loss = 0.04504307\n",
      "Iteration 50, loss = 0.05905562\n",
      "Iteration 51, loss = 0.06161404\n",
      "Iteration 52, loss = 0.06555963\n",
      "Iteration 53, loss = 0.04542896\n",
      "Iteration 54, loss = 0.06864970\n",
      "Iteration 55, loss = 0.07065580\n",
      "Iteration 56, loss = 0.04568067\n",
      "Iteration 57, loss = 0.07648928\n",
      "Iteration 58, loss = 0.03330870\n",
      "Iteration 59, loss = 0.03693946\n",
      "Iteration 60, loss = 0.06935452\n",
      "Iteration 61, loss = 0.04032776\n",
      "Iteration 62, loss = 0.06699149\n",
      "Iteration 63, loss = 0.06200600\n",
      "Iteration 64, loss = 0.02944825\n",
      "Iteration 65, loss = 0.02319356\n",
      "Iteration 66, loss = 0.02963084\n",
      "Iteration 67, loss = 0.08248009\n",
      "Iteration 68, loss = 0.09732542\n",
      "Iteration 69, loss = 0.05110761\n",
      "Iteration 70, loss = 0.04319830\n",
      "Iteration 71, loss = 0.05721357\n",
      "Iteration 72, loss = 0.06305638\n",
      "Iteration 73, loss = 0.03261564\n",
      "Iteration 74, loss = 0.04602418\n",
      "Iteration 75, loss = 0.03840193\n",
      "Iteration 76, loss = 0.04375102\n",
      "Iteration 77, loss = 0.03882382\n",
      "Iteration 78, loss = 0.03529449\n",
      "Iteration 79, loss = 0.04975886\n",
      "Iteration 80, loss = 0.04654851\n",
      "-----------Finished lr 100 with score 0.9630952380952381\n",
      "-----------Starting training with lr (20, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Felipe\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (80) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 5.37796498\n",
      "Iteration 2, loss = 1.88288373\n",
      "Iteration 3, loss = 1.66145013\n",
      "Iteration 4, loss = 1.52197386\n",
      "Iteration 5, loss = 1.43069066\n",
      "Iteration 6, loss = 1.36084339\n",
      "Iteration 7, loss = 1.28264130\n",
      "Iteration 8, loss = 1.21824234\n",
      "Iteration 9, loss = 1.14936803\n",
      "Iteration 10, loss = 1.08493784\n",
      "Iteration 11, loss = 1.05027312\n",
      "Iteration 12, loss = 1.01327585\n",
      "Iteration 13, loss = 0.97116662\n",
      "Iteration 14, loss = 0.93667574\n",
      "Iteration 15, loss = 0.80650160\n",
      "Iteration 16, loss = 0.72566648\n",
      "Iteration 17, loss = 0.66646423\n",
      "Iteration 18, loss = 0.63043847\n",
      "Iteration 19, loss = 0.59619426\n",
      "Iteration 20, loss = 0.56389703\n",
      "Iteration 21, loss = 0.53846401\n",
      "Iteration 22, loss = 0.51735129\n",
      "Iteration 23, loss = 0.49437060\n",
      "Iteration 24, loss = 0.47040671\n",
      "Iteration 25, loss = 0.46020348\n",
      "Iteration 26, loss = 0.43466796\n",
      "Iteration 27, loss = 0.42085945\n",
      "Iteration 28, loss = 0.41013924\n",
      "Iteration 29, loss = 0.39098277\n",
      "Iteration 30, loss = 0.37658932\n",
      "Iteration 31, loss = 0.37192073\n",
      "Iteration 32, loss = 0.35915710\n",
      "Iteration 33, loss = 0.34367931\n",
      "Iteration 34, loss = 0.33341984\n",
      "Iteration 35, loss = 0.32721168\n",
      "Iteration 36, loss = 0.32069391\n",
      "Iteration 37, loss = 0.31236764\n",
      "Iteration 38, loss = 0.31019898\n",
      "Iteration 39, loss = 0.30114467\n",
      "Iteration 40, loss = 0.29587928\n",
      "Iteration 41, loss = 0.28391129\n",
      "Iteration 42, loss = 0.28236749\n",
      "Iteration 43, loss = 0.27857816\n",
      "Iteration 44, loss = 0.27415631\n",
      "Iteration 45, loss = 0.26838160\n",
      "Iteration 46, loss = 0.26524591\n",
      "Iteration 47, loss = 0.26044236\n",
      "Iteration 48, loss = 0.25054433\n",
      "Iteration 49, loss = 0.24983564\n",
      "Iteration 50, loss = 0.25091087\n",
      "Iteration 51, loss = 0.24993391\n",
      "Iteration 52, loss = 0.23955840\n",
      "Iteration 53, loss = 0.23507744\n",
      "Iteration 54, loss = 0.23401598\n",
      "Iteration 55, loss = 0.23187426\n",
      "Iteration 56, loss = 0.22939211\n",
      "Iteration 57, loss = 0.22351688\n",
      "Iteration 58, loss = 0.22227805\n",
      "Iteration 59, loss = 0.22252556\n",
      "Iteration 60, loss = 0.21548999\n",
      "Iteration 61, loss = 0.21688874\n",
      "Iteration 62, loss = 0.20901512\n",
      "Iteration 63, loss = 0.21182208\n",
      "Iteration 64, loss = 0.21053382\n",
      "Iteration 65, loss = 0.20262081\n",
      "Iteration 66, loss = 0.20236249\n",
      "Iteration 67, loss = 0.19703030\n",
      "Iteration 68, loss = 0.19637420\n",
      "Iteration 69, loss = 0.19221540\n",
      "Iteration 70, loss = 0.18621209\n",
      "Iteration 71, loss = 0.19049193\n",
      "Iteration 72, loss = 0.18964191\n",
      "Iteration 73, loss = 0.18575757\n",
      "Iteration 74, loss = 0.17894122\n",
      "Iteration 75, loss = 0.17748301\n",
      "Iteration 76, loss = 0.17916056\n",
      "Iteration 77, loss = 0.17610946\n",
      "Iteration 78, loss = 0.18088993\n",
      "Iteration 79, loss = 0.16862304\n",
      "Iteration 80, loss = 0.16943429\n",
      "-----------Finished lr (20, 10) with score 0.9137301587301587\n",
      "-----------Starting training with lr (30, 20, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Felipe\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (80) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 3.19995980\n",
      "Iteration 2, loss = 1.25213142\n",
      "Iteration 3, loss = 0.90707957\n",
      "Iteration 4, loss = 0.70432685\n",
      "Iteration 5, loss = 0.59491343\n",
      "Iteration 6, loss = 0.52851372\n",
      "Iteration 7, loss = 0.47636893\n",
      "Iteration 8, loss = 0.42845556\n",
      "Iteration 9, loss = 0.39029889\n",
      "Iteration 10, loss = 0.37044696\n",
      "Iteration 11, loss = 0.34068150\n",
      "Iteration 12, loss = 0.32544436\n",
      "Iteration 13, loss = 0.30509389\n",
      "Iteration 14, loss = 0.29483925\n",
      "Iteration 15, loss = 0.28080200\n",
      "Iteration 16, loss = 0.26580297\n",
      "Iteration 17, loss = 0.25951228\n",
      "Iteration 18, loss = 0.24947619\n",
      "Iteration 19, loss = 0.23922325\n",
      "Iteration 20, loss = 0.23197527\n",
      "Iteration 21, loss = 0.22344993\n",
      "Iteration 22, loss = 0.22152229\n",
      "Iteration 23, loss = 0.21365853\n",
      "Iteration 24, loss = 0.20788750\n",
      "Iteration 25, loss = 0.20860865\n",
      "Iteration 26, loss = 0.19322361\n",
      "Iteration 27, loss = 0.18753729\n",
      "Iteration 28, loss = 0.18174643\n",
      "Iteration 29, loss = 0.18099395\n",
      "Iteration 30, loss = 0.17759237\n",
      "Iteration 31, loss = 0.17341455\n",
      "Iteration 32, loss = 0.16570806\n",
      "Iteration 33, loss = 0.16185049\n",
      "Iteration 34, loss = 0.15635801\n",
      "Iteration 35, loss = 0.15717573\n",
      "Iteration 36, loss = 0.15361001\n",
      "Iteration 37, loss = 0.15112191\n",
      "Iteration 38, loss = 0.15262960\n",
      "Iteration 39, loss = 0.14377128\n",
      "Iteration 40, loss = 0.13596906\n",
      "Iteration 41, loss = 0.14105462\n",
      "Iteration 42, loss = 0.14399743\n",
      "Iteration 43, loss = 0.13793077\n",
      "Iteration 44, loss = 0.13125396\n",
      "Iteration 45, loss = 0.12566516\n",
      "Iteration 46, loss = 0.12827504\n",
      "Iteration 47, loss = 0.12170448\n",
      "Iteration 48, loss = 0.12167554\n",
      "Iteration 49, loss = 0.12217161\n",
      "Iteration 50, loss = 0.12184506\n",
      "Iteration 51, loss = 0.11966219\n",
      "Iteration 52, loss = 0.11575476\n",
      "Iteration 53, loss = 0.11196005\n",
      "Iteration 54, loss = 0.11351045\n",
      "Iteration 55, loss = 0.11824068\n",
      "Iteration 56, loss = 0.11014345\n",
      "Iteration 57, loss = 0.10933847\n",
      "Iteration 58, loss = 0.10680592\n",
      "Iteration 59, loss = 0.10646607\n",
      "Iteration 60, loss = 0.10610995\n",
      "Iteration 61, loss = 0.10187055\n",
      "Iteration 62, loss = 0.10398700\n",
      "Iteration 63, loss = 0.10060823\n",
      "Iteration 64, loss = 0.10168495\n",
      "Iteration 65, loss = 0.09777508\n",
      "Iteration 66, loss = 0.09514360\n",
      "Iteration 67, loss = 0.09523919\n",
      "Iteration 68, loss = 0.09914830\n",
      "Iteration 69, loss = 0.09056712\n",
      "Iteration 70, loss = 0.08726089\n",
      "Iteration 71, loss = 0.09175182\n",
      "Iteration 72, loss = 0.08968513\n",
      "Iteration 73, loss = 0.09410638\n",
      "Iteration 74, loss = 0.08734939\n",
      "Iteration 75, loss = 0.09081154\n",
      "Iteration 76, loss = 0.08519292\n",
      "Iteration 77, loss = 0.08222844\n",
      "Iteration 78, loss = 0.08381795\n",
      "Iteration 79, loss = 0.08079490\n",
      "Iteration 80, loss = 0.09423780\n",
      "-----------Finished lr (30, 20, 10) with score 0.9315873015873016\n",
      "-----------Starting training with lr (100, 30, 20, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Felipe\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (80) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 3.12481554\n",
      "Iteration 2, loss = 1.14091593\n",
      "Iteration 3, loss = 0.83582234\n",
      "Iteration 4, loss = 0.55727279\n",
      "Iteration 5, loss = 0.38138987\n",
      "Iteration 6, loss = 0.29706384\n",
      "Iteration 7, loss = 0.23998337\n",
      "Iteration 8, loss = 0.20768927\n",
      "Iteration 9, loss = 0.18221275\n",
      "Iteration 10, loss = 0.17088626\n",
      "Iteration 11, loss = 0.13527509\n",
      "Iteration 12, loss = 0.12912826\n",
      "Iteration 13, loss = 0.10910068\n",
      "Iteration 14, loss = 0.10089850\n",
      "Iteration 15, loss = 0.09657211\n",
      "Iteration 16, loss = 0.09330860\n",
      "Iteration 17, loss = 0.08313284\n",
      "Iteration 18, loss = 0.06835692\n",
      "Iteration 19, loss = 0.06487736\n",
      "Iteration 20, loss = 0.06310378\n",
      "Iteration 21, loss = 0.06426749\n",
      "Iteration 22, loss = 0.06353525\n",
      "Iteration 23, loss = 0.05415676\n",
      "Iteration 24, loss = 0.05116692\n",
      "Iteration 25, loss = 0.05750300\n",
      "Iteration 26, loss = 0.05026858\n",
      "Iteration 27, loss = 0.04011395\n",
      "Iteration 28, loss = 0.04663917\n",
      "Iteration 29, loss = 0.05186012\n",
      "Iteration 30, loss = 0.04645203\n",
      "Iteration 31, loss = 0.03586358\n",
      "Iteration 32, loss = 0.03419706\n",
      "Iteration 33, loss = 0.04644933\n",
      "Iteration 34, loss = 0.03646148\n",
      "Iteration 35, loss = 0.02697687\n",
      "Iteration 36, loss = 0.03657996\n",
      "Iteration 37, loss = 0.03067104\n",
      "Iteration 38, loss = 0.03384640\n",
      "Iteration 39, loss = 0.03037047\n",
      "Iteration 40, loss = 0.03120644\n",
      "Iteration 41, loss = 0.02028095\n",
      "Iteration 42, loss = 0.03292946\n",
      "Iteration 43, loss = 0.02998592\n",
      "Iteration 44, loss = 0.03451970\n",
      "Iteration 45, loss = 0.01963968\n",
      "Iteration 46, loss = 0.01462884\n",
      "Iteration 47, loss = 0.01689150\n",
      "Iteration 48, loss = 0.02003685\n",
      "Iteration 49, loss = 0.01779206\n",
      "Iteration 50, loss = 0.03430293\n",
      "Iteration 51, loss = 0.03237394\n",
      "Iteration 52, loss = 0.02297532\n",
      "Iteration 53, loss = 0.00983752\n",
      "Iteration 54, loss = 0.01461378\n",
      "Iteration 55, loss = 0.01902436\n",
      "Iteration 56, loss = 0.01710350\n",
      "Iteration 57, loss = 0.02106398\n",
      "Iteration 58, loss = 0.02699299\n",
      "Iteration 59, loss = 0.02024416\n",
      "Iteration 60, loss = 0.01119497\n",
      "Iteration 61, loss = 0.01897058\n",
      "Iteration 62, loss = 0.02339999\n",
      "Iteration 63, loss = 0.01935061\n",
      "Iteration 64, loss = 0.00841155\n",
      "Iteration 65, loss = 0.00722221\n",
      "Iteration 66, loss = 0.01766804\n",
      "Iteration 67, loss = 0.03047041\n",
      "Iteration 68, loss = 0.01650161\n",
      "Iteration 69, loss = 0.01426326\n",
      "Iteration 70, loss = 0.00788658\n",
      "Iteration 71, loss = 0.00619239\n",
      "Iteration 72, loss = 0.00979892\n",
      "Iteration 73, loss = 0.02763123\n",
      "Iteration 74, loss = 0.01989945\n",
      "Iteration 75, loss = 0.01617558\n",
      "Iteration 76, loss = 0.00693945\n",
      "Iteration 77, loss = 0.00485311\n",
      "Iteration 78, loss = 0.00397994\n",
      "Iteration 79, loss = 0.00302811\n",
      "Iteration 80, loss = 0.00324025\n",
      "-----------Finished lr (100, 30, 20, 10) with score 0.9644444444444444"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Felipe\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (80) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "layers = [(100), (20, 10), (30, 20, 10), (100, 30, 20, 10)]\n",
    "scores = []\n",
    "\n",
    "for lyr in layers:\n",
    "    print(f\"-----------Starting training with lr {lyr}\")\n",
    "    model = MLPClassifier(hidden_layer_sizes = lyr, verbose = True, max_iter=80, n_iter_no_change = 20)\n",
    "    model.fit(x_train, y_train)\n",
    "    score = model.score(x_test, y_test)\n",
    "    scores.append(score)\n",
    "    print(f\"-----------Finished lr {lyr} with score {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that all results are similar, let's try something a bit deeper. (takes long to execute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = MLPClassifier(hidden_layer_sizes = (2500, 2000, 1500, 1000, 500), verbose = True, max_iter = 200, n_iter_no_change = 20, tol = 0.00005)\n",
    "#model.fit(x_train, y_train)\n",
    "#model.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to look for the right params, and have it not be so problematic, let's use an already defined scikit-learn function called **GridSearchCV**. This function will help us by making all the combinations of the parameters we give it and using them in the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "model = MLPClassifier(verbose = True)\n",
    "\n",
    "parameters = {\n",
    "    \"hidden_layer_sizes\": [(100), (20, 10), (30, 20, 10)],\n",
    "    \"learning_rate_init\": [0.0009, 0.001, 0.002],\n",
    "    \"batch_size\": [30000, 1000, 200, 1],\n",
    "    \"max_iter\": [100],\n",
    "    \"tol\": [0.00001, 0.000005]\n",
    "}\n",
    "\n",
    "search = GridSearchCV(model, parameters, cv = 5)\n",
    "search.fit(data.drop(\"label\", axis = 1), data.label)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "nnet-class.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
